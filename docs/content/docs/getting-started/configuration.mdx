---
title: Configuration
description: Learn how to configure OramaCore
---

OramaCore is currently under active development. Our goal is to release the first Beta version (**v0.1.0**) on **Jan 31st, 2025**, and the first stable version (**v1.0.0**) on **Feb 28th, 2025**.

While the system is already quite stable, please note that APIs will undergo changes in **v0.1.0** and **v1.0.0**.

## Configuring OramaCore

OramaCore uses a straightforward `config.yaml` file for configuration and customization.

While we provide robust defaults for nearly every option in the configuration file — making it almost unnecessary to write one yourself — here is an example of a typical `config.yaml` file:

```yaml
http:
    host: 0.0.0.0
    port: 8080
    allow_cors: true
    with_prometheus: true

writer_side:
    output: in-memory
    config:
        data_dir: ./.data/writer
        # The maximum number of embeddings that can be stored in the queue
        # before the writer starts to be blocked
        # NB: the elements are in memory, so be careful with this value
        embedding_queue_limit: 50
        # The number of the document insertions after the write side will commit the changes
        insert_batch_commit_size: 5000
        # The default embedding model used to calculate the embeddings
        # if not specified in the collection creation
        default_embedding_model: MultilingualE5Small

reader_side:
    input: in-memory
    config:
        data_dir: ./.data/reader
        # The number of the write operation after the read side will commit the changes
        insert_batch_commit_size: 300

ai_server:
    scheme: http
    host: 0.0.0.0
    port: 50051
    api_key: ""
    max_connections: 15
    total_threads: 12

    embeddings:
        default_model_group: small
        dynamically_load_models: false
        execution_providers:
            - CUDAExecutionProvider
            - CPUExecutionProvider
        total_threads: 8
    LLMs:
        google_query_translator:
            id: "Qwen/Qwen2.5-3B-Instruct"
            tensor_parallel_size: 1
            use_cpu: false
            sampling_params:
                temperature: 0.2
                top_p: 0.95
                max_tokens: 20
        answer:
            id: "Qwen/Qwen2.5-3B-Instruct"
            tensor_parallel_size: 1
            use_cpu: false
            sampling_params:
                temperature: 0
                top_p: 0.95
                max_tokens: 2048
```